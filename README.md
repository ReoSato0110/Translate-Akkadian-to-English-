## 目的
- Kaggle Deep Past Challengeでのメダル獲得
- 再現可能なDeep Learningパイプラインの構築

## アプローチ概要
- ベースラインとしてbyt5-baseモデルを採用
- 前処理 → 学習 → 評価 → 推論を一貫したコードで管理
- 再現性を最優先（seed）
→構造・ソースをローカルで整理し、実行・実験をkaggle notebookで行う。  

<推移>  
1.ベースラインを文単位にして、評価対象のtestに合わせる。
2.
## 学び・仮説
- 評価指標のBLEUは、「どれだけ“連続した単語の塊”が一致しているか」。*意味は見ない
  - 語順
  - 機能語（a / the / of / to / is / was など）
  - 定型フレーズ
  - 表記の癖（記号・長音・特殊文字）
---
- 短い文ズル対策：Brevity Penalty
  　　短ければ一致率が高くなる逃げを防ぐ。

- Dice係数（辞書確率）:強い対応関係を明確化する。(not 頻度)

  (例：score = 2 * c / (src_cnt[sw] + tgt_cnt[tw]
  
  「このアッカド語単語 sw がこの英語単語 tw と一緒に出る確率」を測っている。)

- GAP処理：以下のような欠損文字をgap,big_gapに統一することで、ノイズを意味あるトークンへ変換。欠損の大小を考えられる。

```markdown
欠損文字  
x  
...  
xx  
```
↓
```markdown 
<gap>  
<big_gap>
```

---


### <上位者notebook/discussion>
- コンペ：定型句が多い、似た石版が大量にある
→類似文検索の相性が良く、ルールベースのTF-IDF検索が用いられている。
- byT5系のfine-tuningモデルとbyt5-largeなどの複数モデル。（LLMや外部データ使用なども考慮）
- 正規化・Sentence-level化・固有名詞保全・長さ設計・生成安定化、実行環境最適化etc...
---

### 更新履歴
- 2/22：submission.csv ＝　score 3.4でlossが1.6-2.1。　学習データが足りない可能性が高い。
- 2/22：文単位にtrainを分割実装＋Group K foldで実装。（fp16を導入しようとして失敗。lossがNaNになってしまう）
